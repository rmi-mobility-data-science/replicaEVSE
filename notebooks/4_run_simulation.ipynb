{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a sandbox for setting up a run of the simulation on `PRIVATE_AUTO` only. \n",
    "\n",
    "* It can be run on a small subsample pandas DataFrame\n",
    "* The dask implementation might work with `map_partition()` but also might not. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "import numpy as np\n",
    "import replicaEVSE.load_curve as sim\n",
    "import os\n",
    "from dask.diagnostics import ProgressBar\n",
    "import joblib\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "datadir = '../../data'\n",
    "\n",
    "\n",
    "\n",
    "#Created in the EIA_data_download.ipynb notebook\n",
    "existing_load=pd.read_csv('../../data/EIA_demand_summary.csv') \n",
    "\n",
    "# read in the joined trips and population data sets\n",
    "merged_ddf = dd.read_parquet(os.path.join(datadir, 'wa_pop_and_trips.parquet'))\n",
    "\n",
    "# right now, only look at private auto trips\n",
    "ddf = merged_ddf.loc[merged_ddf['mode'] == 'PRIVATE_AUTO']\n",
    "\n",
    "# This is necessary for map partitions. Perhaps we save a new \n",
    "# version of the parquet files where this is true and \n",
    "# the data are presorted. The simulation does some sorting however.\n",
    "# would this make it faster?\n",
    "ddf = ddf.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# sort on person_id and start_time\n",
    "#ddf = ddf.sort_values(by=['person_id', 'start_time', 'weekday']).reset_index(drop=True)\n",
    "\n",
    "df = merged_ddf.head(10000)\n",
    "\n",
    "small_ddf = dd.from_pandas(df, npartitions=10)\n",
    "\n",
    "# to run the whole data set, uncomment the following line\n",
    "# df = ddf.compute()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start a dask cluster to get diagnostic info. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "cluster = LocalCluster(n_workers=8)  # Launches a scheduler and workers locally\n",
    "client = Client(cluster)  # Connect to distributed cluster and override default\n",
    "\n",
    "pbar = ProgressBar()\n",
    "pbar.register()\n",
    "\n",
    "client"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run the simulation on small dataset to test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this takes ~1min for 10,000 trips\n",
    "out = sim.simulate_person_load(df, existing_load, 'base', managed=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a dask implementation that would be significantly faster or can do the embarrassingly parallel option below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to use dask\n",
    "# not tested. \n",
    "# out = small_ddf.map_partitions(sim.simulate_person_load, existing_load, 'base', managed=False, align_dataframes=False)\n",
    "# out.compute()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embarrassingly parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_chunks = 1000\n",
    "df_list = np.array_split(df, number_of_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the simulation in parallel\n",
    "# df must be a pandas dataframe\n",
    "charge_sims = joblib.Parallel(verbose=10, n_jobs=-1)(\n",
    "    joblib.delayed(sim.simulate_person_load)(\n",
    "    df=df,\n",
    "    existing_load=existing_load,\n",
    "    simulation_id='base',\n",
    "    managed=False\n",
    ") for df in df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restack the dataframes\n",
    "charges_list = [x['charges'] for x in charge_sims]\n",
    "charges_df = dd.concat(charges_list)\n",
    "\n",
    "loads_list = [x['loads'] for x in charge_sims]\n",
    "loads_df = dd.concat(loads_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# below this is just exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac = 0.001 # ~2e4\n",
    "thu = dd.read_parquet(os.path.join(datadir, 'northwest_2021_Q4_thursday_trip.parquet'))\n",
    "sat = dd.read_parquet(os.path.join(datadir, 'northwest_2021_Q4_saturday_trip.parquet'))\n",
    "pop = dd.read_parquet(os.path.join(datadir, 'northwest_2021_Q4_population.parquet'))\n",
    "ddf = dd.read_parquet(os.path.join(datadir, 'wa_pop_and_trips.parquet'))\n",
    "# df = ddf.head(10000000)\n",
    "# trips_ddf = dd.concat([thu, sat])\n",
    "# ddf = dd.from_pandas(df, chunksize=1000)\n",
    "# df['mode'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trips_ddf.loc[trips_ddf['mode'] == 'PRIVATE_AUTO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sat) + len(thu), len(ddf), len(pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_len = 58.222322\n",
    "trips_len = 159.624453 # million\n",
    "join_len = 51.727268 \n",
    "other_len = 49.674863\n",
    "pop_len = 14.889896"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_people_in_pop_len = len(pop['person_id'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trucks = trips_ddf.loc[trips_ddf['mode'] == 'COMMERCIAL']\n",
    "other = trips_ddf.loc[trips_ddf['mode'] != 'PRIVATE_AUTO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trucks['person_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trucks.loc[trucks['vehicle_type'] == 'MEDIUM_COMMERCIAL']['distance_miles'].hist(bins=100,  alpha=0.5)\n",
    "trucks.loc[trucks['vehicle_type'] == 'HEAVY_COMMERCIAL']['distance_miles'].hist(bins=100, log=True, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips = dd.read_parquet(os.path.join(datadir, 'northwest_2021_Q4_thursday_trip.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
