{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean the data and make parquet files\n",
    "\n",
    " The files we were given for the northwest region from replica have header rows embedded through out due to the way the google cloud on their end stacks things.\n",
    "\n",
    " Here we clean convert every column to strings and then remove those rows and save the data to parquet files which are easier to deal with.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from replicaEVSE import datautils as du\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### path to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '../../data/'\n",
    "popfile = 'northwest_2021_Q4_population.csv'\n",
    "tripsatfile = 'northwest_2021_Q4_saturday_trip.csv'\n",
    "tripthufile = 'northwest_2021_Q4_thursday_trip.csv'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load and convert all the data to strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_df = du.load_data(os.path.join(datapath, popfile))\n",
    "tripsat_df = du.load_data(os.path.join(datapath, tripsatfile))\n",
    "tripthu_df = du.load_data(os.path.join(datapath, tripthufile))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean the data of bad rows (embedded headers in the data) and return the dask data frames for inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m pop_df \u001b[39m=\u001b[39m du\u001b[39m.\u001b[39mclean_pop_data(pop_df)\n\u001b[1;32m      2\u001b[0m tripsat_df \u001b[39m=\u001b[39m du\u001b[39m.\u001b[39mclean_trip_data(tripsat_df)\n\u001b[0;32m----> 3\u001b[0m tripthu_df \u001b[39m=\u001b[39m du\u001b[39m.\u001b[39;49mclean_trip_data(tripthu_df)\n",
      "File \u001b[0;32m~/Projects/WATES/replicaEVSE/src/replicaEVSE/datautils.py:66\u001b[0m, in \u001b[0;36mclean_trip_data\u001b[0;34m(trip_df)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m trip_df\u001b[39m.\u001b[39mcolumns:\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m col \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mperson_id\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         trip_dict[col] \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m\n\u001b[1;32m     67\u001b[0m     \u001b[39mif\u001b[39;00m col \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mstart_time\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mend_time\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m     68\u001b[0m         trip_dict[col] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtimedelta64[ns]\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[0;32m~/mambaforge/envs/py311/lib/python3.11/site-packages/dask/dataframe/core.py:5199\u001b[0m, in \u001b[0;36mDataFrame.set_index\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   5196\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   5197\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mdask\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataframe\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mshuffle\u001b[39;00m \u001b[39mimport\u001b[39;00m set_index\n\u001b[0;32m-> 5199\u001b[0m     \u001b[39mreturn\u001b[39;00m set_index(\n\u001b[1;32m   5200\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[1;32m   5201\u001b[0m         other,\n\u001b[1;32m   5202\u001b[0m         drop\u001b[39m=\u001b[39;49mdrop,\n\u001b[1;32m   5203\u001b[0m         npartitions\u001b[39m=\u001b[39;49mnpartitions,\n\u001b[1;32m   5204\u001b[0m         divisions\u001b[39m=\u001b[39;49mdivisions,\n\u001b[1;32m   5205\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   5206\u001b[0m     )\n",
      "File \u001b[0;32m~/mambaforge/envs/py311/lib/python3.11/site-packages/dask/dataframe/shuffle.py:229\u001b[0m, in \u001b[0;36mset_index\u001b[0;34m(df, index, npartitions, shuffle, compute, drop, upsample, divisions, partition_size, **kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m     index2 \u001b[39m=\u001b[39m index\n\u001b[1;32m    228\u001b[0m \u001b[39mif\u001b[39;00m divisions \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 229\u001b[0m     divisions, mins, maxes, presorted \u001b[39m=\u001b[39m _calculate_divisions(\n\u001b[1;32m    230\u001b[0m         df, index2, repartition, npartitions, upsample, partition_size\n\u001b[1;32m    231\u001b[0m     )\n\u001b[1;32m    233\u001b[0m     \u001b[39mif\u001b[39;00m presorted \u001b[39mand\u001b[39;00m npartitions \u001b[39m==\u001b[39m df\u001b[39m.\u001b[39mnpartitions:\n\u001b[1;32m    234\u001b[0m         divisions \u001b[39m=\u001b[39m mins \u001b[39m+\u001b[39m [maxes[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]]\n",
      "File \u001b[0;32m~/mambaforge/envs/py311/lib/python3.11/site-packages/dask/dataframe/shuffle.py:48\u001b[0m, in \u001b[0;36m_calculate_divisions\u001b[0;34m(df, partition_col, repartition, npartitions, upsample, partition_size, ascending)\u001b[0m\n\u001b[1;32m     45\u001b[0m maxes \u001b[39m=\u001b[39m partition_col\u001b[39m.\u001b[39mmap_partitions(M\u001b[39m.\u001b[39mmax)\n\u001b[1;32m     47\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 48\u001b[0m     divisions, sizes, mins, maxes \u001b[39m=\u001b[39m compute(divisions, sizes, mins, maxes)\n\u001b[1;32m     49\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     50\u001b[0m     \u001b[39m# When there are nulls and a column is non-numeric, a TypeError is sometimes raised as a result of\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     \u001b[39m# 1) computing mins/maxes above, 2) every null being switched to NaN, and 3) NaN being a float.\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     \u001b[39m# Also, Pandas ExtensionDtypes may cause TypeErrors when dealing with special nulls such as pd.NaT or pd.NA.\u001b[39;00m\n\u001b[1;32m     53\u001b[0m     \u001b[39m# If this happens, we hint the user about eliminating nulls beforehand.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_numeric_dtype(partition_col\u001b[39m.\u001b[39mdtype):\n",
      "File \u001b[0;32m~/mambaforge/envs/py311/lib/python3.11/site-packages/dask/base.py:599\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    596\u001b[0m     keys\u001b[39m.\u001b[39mappend(x\u001b[39m.\u001b[39m__dask_keys__())\n\u001b[1;32m    597\u001b[0m     postcomputes\u001b[39m.\u001b[39mappend(x\u001b[39m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m--> 599\u001b[0m results \u001b[39m=\u001b[39m schedule(dsk, keys, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    600\u001b[0m \u001b[39mreturn\u001b[39;00m repack([f(r, \u001b[39m*\u001b[39ma) \u001b[39mfor\u001b[39;00m r, (f, a) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m~/mambaforge/envs/py311/lib/python3.11/site-packages/dask/threaded.py:89\u001b[0m, in \u001b[0;36mget\u001b[0;34m(dsk, keys, cache, num_workers, pool, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(pool, multiprocessing\u001b[39m.\u001b[39mpool\u001b[39m.\u001b[39mPool):\n\u001b[1;32m     87\u001b[0m         pool \u001b[39m=\u001b[39m MultiprocessingPoolExecutor(pool)\n\u001b[0;32m---> 89\u001b[0m results \u001b[39m=\u001b[39m get_async(\n\u001b[1;32m     90\u001b[0m     pool\u001b[39m.\u001b[39;49msubmit,\n\u001b[1;32m     91\u001b[0m     pool\u001b[39m.\u001b[39;49m_max_workers,\n\u001b[1;32m     92\u001b[0m     dsk,\n\u001b[1;32m     93\u001b[0m     keys,\n\u001b[1;32m     94\u001b[0m     cache\u001b[39m=\u001b[39;49mcache,\n\u001b[1;32m     95\u001b[0m     get_id\u001b[39m=\u001b[39;49m_thread_get_id,\n\u001b[1;32m     96\u001b[0m     pack_exception\u001b[39m=\u001b[39;49mpack_exception,\n\u001b[1;32m     97\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m     98\u001b[0m )\n\u001b[1;32m    100\u001b[0m \u001b[39m# Cleanup pools associated to dead threads\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[39mwith\u001b[39;00m pools_lock:\n",
      "File \u001b[0;32m~/mambaforge/envs/py311/lib/python3.11/site-packages/dask/local.py:500\u001b[0m, in \u001b[0;36mget_async\u001b[0;34m(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[39mwhile\u001b[39;00m state[\u001b[39m\"\u001b[39m\u001b[39mwaiting\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mor\u001b[39;00m state[\u001b[39m\"\u001b[39m\u001b[39mready\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mor\u001b[39;00m state[\u001b[39m\"\u001b[39m\u001b[39mrunning\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    499\u001b[0m     fire_tasks(chunksize)\n\u001b[0;32m--> 500\u001b[0m     \u001b[39mfor\u001b[39;00m key, res_info, failed \u001b[39min\u001b[39;00m queue_get(queue)\u001b[39m.\u001b[39mresult():\n\u001b[1;32m    501\u001b[0m         \u001b[39mif\u001b[39;00m failed:\n\u001b[1;32m    502\u001b[0m             exc, tb \u001b[39m=\u001b[39m loads(res_info)\n",
      "File \u001b[0;32m~/mambaforge/envs/py311/lib/python3.11/site-packages/dask/local.py:137\u001b[0m, in \u001b[0;36mqueue_get\u001b[0;34m(q)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mqueue_get\u001b[39m(q):\n\u001b[0;32m--> 137\u001b[0m     \u001b[39mreturn\u001b[39;00m q\u001b[39m.\u001b[39;49mget()\n",
      "File \u001b[0;32m~/mambaforge/envs/py311/lib/python3.11/queue.py:171\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39melif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_qsize():\n\u001b[0;32m--> 171\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnot_empty\u001b[39m.\u001b[39;49mwait()\n\u001b[1;32m    172\u001b[0m \u001b[39melif\u001b[39;00m timeout \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    173\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m'\u001b[39m\u001b[39m must be a non-negative number\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/mambaforge/envs/py311/lib/python3.11/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pop_df = du.clean_pop_data(pop_df)\n",
    "tripsat_df = du.clean_trip_data(tripsat_df)\n",
    "tripthu_df = du.clean_trip_data(tripthu_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or we can use our wrapper to clean save the data in the parquet format to speed analysis later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "popparquet = 'northwest_2021_Q4_population.parquet'\n",
    "tripsatparquet = 'northwest_2021_Q4_saturday_trip.parquet'\n",
    "tripthuparquet = 'northwest_2021_Q4_thursday_trip.parquet'\n",
    "pop_df = pop_df.to_parquet(os.path.join(datapath, popparquet))\n",
    "tripsat_df = tripsat_df.to_parquet(os.path.join(datapath, tripsatparquet))\n",
    "tripthu_df = tripthu_df.to_parquet(os.path.join(datapath, tripthuparquet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
