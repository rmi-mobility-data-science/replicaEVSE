{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean the data and make parquet files\n",
    "\n",
    " The files we were given for the northwest region from replica have header rows embedded through out due to the way the google cloud on their end stacks things.\n",
    "\n",
    " Here we clean convert every column to strings and then remove those rows and save the data to parquet files which are easier to deal with.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from replicaEVSE import datautils as du\n",
    "import os\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "\n",
    "datadir = '../../data/'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### path to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '../../data/'\n",
    "popfile = 'northwest_2021_Q4_population.csv'\n",
    "tripsatfile = 'northwest_2021_Q4_saturday_trip.csv'\n",
    "tripthufile = 'northwest_2021_Q4_thursday_trip.csv'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load and convert all the data to strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_df = du.load_data(os.path.join(datapath, popfile))\n",
    "tripsat_df = du.load_data(os.path.join(datapath, tripsatfile))\n",
    "tripthu_df = du.load_data(os.path.join(datapath, tripthufile))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean the data of bad rows (embedded headers in the data) and return the dask data frames for inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_df = du.clean_pop_data(pop_df)\n",
    "tripsat_df = du.clean_trip_data(tripsat_df)\n",
    "tripthu_df = du.clean_trip_data(tripthu_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or we can use our wrapper to clean save the data in the parquet format to speed analysis later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pop_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m tripsatparquet \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mnorthwest_2021_Q4_saturday_trip.parquet\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      3\u001b[0m tripthuparquet \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mnorthwest_2021_Q4_thursday_trip.parquet\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 4\u001b[0m pop_df \u001b[39m=\u001b[39m pop_df\u001b[39m.\u001b[39mto_parquet(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(datapath, popparquet))\n\u001b[1;32m      5\u001b[0m tripsat_df \u001b[39m=\u001b[39m tripsat_df\u001b[39m.\u001b[39mto_parquet(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(datapath, tripsatparquet))\n\u001b[1;32m      6\u001b[0m tripthu_df \u001b[39m=\u001b[39m tripthu_df\u001b[39m.\u001b[39mto_parquet(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(datapath, tripthuparquet))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pop_df' is not defined"
     ]
    }
   ],
   "source": [
    "popparquet = 'northwest_2021_Q4_population.parquet'\n",
    "tripsatparquet = 'northwest_2021_Q4_saturday_trip.parquet'\n",
    "tripthuparquet = 'northwest_2021_Q4_thursday_trip.parquet'\n",
    "pop_df = pop_df.to_parquet(os.path.join(datapath, popparquet))\n",
    "tripsat_df = tripsat_df.to_parquet(os.path.join(datapath, tripsatparquet))\n",
    "tripthu_df = tripthu_df.to_parquet(os.path.join(datapath, tripthuparquet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "popparquet = 'northwest_2021_Q4_population.parquet'\n",
    "tripthuparquet = 'northwest_2021_Q4_thursday_trip.parquet'\n",
    "df = dd.read_parquet(os.path.join(datapath, tripthuparquet))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a joined table and save as parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of blockgroups\n",
    "gdf = pd.read_pickle(datadir+'/blockgroup_boundaries.pkl')\n",
    "bgrp_list = list(gdf.GEOID.values)\n",
    "\n",
    "trip_sat_ddf = dd.read_parquet(datadir+'/northwest_2021_Q4_saturday_trip.parquet')\n",
    "trip_thu_ddf = dd.read_parquet(datadir+'/northwest_2021_Q4_thursday_trip.parquet')\n",
    "\n",
    "# dtype_dict = {\"person_id\": str, \"home_cty\": \"category\", \"home_st\": \"category\"}\n",
    "dtype_dict = {\"person_id\": str, \"home_cty\": str, \"home_st\": str}\n",
    "counties = dd.read_parquet(datadir+'/population_counties_dataset.parquet', engine='pyarrow')\n",
    "\n",
    "pop_ddf = dd.read_parquet(datadir+'/northwest_2021_Q4_population.parquet')\n",
    "# pop_ddf = dd.merge(pop_ddf, counties, on='person_id', how='left')\n",
    "\n",
    "\n",
    "trip_sat_ddf['weekday'] = 'saturday'\n",
    "trip_thu_ddf['weekday'] = 'thursday'\n",
    "\n",
    "# stack the two dataframes\n",
    "trips = dd.concat([trip_sat_ddf, trip_thu_ddf], axis=0, keys=[\"saturday\", \"thursday\"])\n",
    "\n",
    "\n",
    "# only trips that end in WA\n",
    "trips_ddf = trips.loc[trips['destination_bgrp'].isin(bgrp_list)]\n",
    " \n",
    "merged_ddf = dd.merge(trips_ddf, pop_ddf, on='person_id', how='left')\n",
    "\n",
    "# Create charge_type column from travel_purpose column\n",
    "merged_ddf['charge_type'] = merged_ddf.travel_purpose\n",
    "\n",
    "merged_ddf['charge_type'] = merged_ddf.travel_purpose\n",
    "merged_ddf['charge_type'] = merged_ddf['charge_type'].where(\n",
    "    merged_ddf.charge_type.isin(\n",
    "    ['WORK', 'HOME']), 'PUBLIC')\n",
    "\n",
    "merged_ddf.to_parquet(datadir+'/wa_pop_and_trips.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### make sure the join worked\n",
    "# get list of blockgroups\n",
    "gdf = pd.read_pickle(datadir+'/blockgroup_boundaries.pkl')\n",
    "bgrp_list = list(gdf.GEOID.values)\n",
    "\n",
    "trip_sat_ddf = dd.read_parquet(datadir+'/northwest_2021_Q4_saturday_trip.parquet')\n",
    "trip_thu_ddf = dd.read_parquet(datadir+'/northwest_2021_Q4_thursday_trip.parquet')\n",
    "\n",
    "# dtype_dict = {\"person_id\": str, \"home_cty\": \"category\", \"home_st\": \"category\"}\n",
    "dtype_dict = {\"person_id\": str, \"home_cty\": str, \"home_st\": str}\n",
    "counties = dd.read_parquet(datadir+'/population_counties_dataset.parquet', engine='pyarrow')\n",
    "\n",
    "pop_ddf = dd.read_parquet(datadir+'/northwest_2021_Q4_population.parquet')\n",
    "# pop_ddf = dd.merge(pop_ddf, counties, on='person_id', how='left')\n",
    "\n",
    "\n",
    "trip_sat_ddf['weekday'] = 'saturday'\n",
    "trip_thu_ddf['weekday'] = 'thursday'\n",
    "\n",
    "# stack the two dataframes\n",
    "trips = dd.concat([trip_sat_ddf, trip_thu_ddf], axis=0, keys=[\"saturday\", \"thursday\"])\n",
    "\n",
    "\n",
    "# only trips that end in WA\n",
    "trips_ddf = trips.loc[trips['destination_bgrp'].isin(bgrp_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_into_wa_len = len(trips_ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51727268"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trips_into_wa_len = 51727268"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Passing a 'dask.dataframe.core.Series' to `isin`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# trip persons who are also in population dataset\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m trips_in_pop_ddf \u001b[39m=\u001b[39m trips_ddf\u001b[39m.\u001b[39mloc[trips_ddf[\u001b[39m'\u001b[39;49m\u001b[39mperson_id\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49misin(pop_ddf[\u001b[39m'\u001b[39;49m\u001b[39mperson_id\u001b[39;49m\u001b[39m'\u001b[39;49m])]\n",
      "File \u001b[0;32m~/mambaforge/envs/py311/lib/python3.11/site-packages/dask/dataframe/core.py:4156\u001b[0m, in \u001b[0;36mSeries.isin\u001b[0;34m(self, values)\u001b[0m\n\u001b[1;32m   4153\u001b[0m \u001b[39m@derived_from\u001b[39m(pd\u001b[39m.\u001b[39mSeries)\n\u001b[1;32m   4154\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39misin\u001b[39m(\u001b[39mself\u001b[39m, values):\n\u001b[1;32m   4155\u001b[0m     \u001b[39m# Added just to get the different docstring for Series\u001b[39;00m\n\u001b[0;32m-> 4156\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49misin(values)\n",
      "File \u001b[0;32m~/mambaforge/envs/py311/lib/python3.11/site-packages/dask/dataframe/core.py:3384\u001b[0m, in \u001b[0;36m_Frame.isin\u001b[0;34m(self, values)\u001b[0m\n\u001b[1;32m   3382\u001b[0m     bad_types \u001b[39m=\u001b[39m (_Frame,)\n\u001b[1;32m   3383\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(values, bad_types):\n\u001b[0;32m-> 3384\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mPassing a \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m to `isin`\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m typename(\u001b[39mtype\u001b[39m(values)))\n\u001b[1;32m   3385\u001b[0m meta \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_meta_nonempty\u001b[39m.\u001b[39misin(values)\n\u001b[1;32m   3386\u001b[0m \u001b[39m# We wrap values in a delayed for two reasons:\u001b[39;00m\n\u001b[1;32m   3387\u001b[0m \u001b[39m# - avoid serializing data in every task\u001b[39;00m\n\u001b[1;32m   3388\u001b[0m \u001b[39m# - avoid cost of traversal of large list in optimizations\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Passing a 'dask.dataframe.core.Series' to `isin`"
     ]
    }
   ],
   "source": [
    "# trip persons who are also in population dataset\n",
    "trips_in_pop_ddf = trips_ddf.loc[trips_ddf['person_id'].isin(pop_ddf['person_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14889896"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pop_ddf['person_id'].unique().compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8538399"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trips_ddf['person_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.538399"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8538399/1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mlen\u001b[39;49m(merged_ddf[\u001b[39m'\u001b[39;49m\u001b[39mperson_id\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49munique())\n",
      "File \u001b[0;32m~/mambaforge/envs/py311/lib/python3.11/site-packages/dask/dataframe/core.py:845\u001b[0m, in \u001b[0;36m_Frame.__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    842\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__len__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    843\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction(\n\u001b[1;32m    844\u001b[0m         \u001b[39mlen\u001b[39;49m, np\u001b[39m.\u001b[39;49msum, token\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mlen\u001b[39;49m\u001b[39m\"\u001b[39;49m, meta\u001b[39m=\u001b[39;49m\u001b[39mint\u001b[39;49m, split_every\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[0;32m--> 845\u001b[0m     )\u001b[39m.\u001b[39;49mcompute()\n",
      "File \u001b[0;32m~/mambaforge/envs/py311/lib/python3.11/site-packages/dask/base.py:314\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    291\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \n\u001b[1;32m    293\u001b[0m \u001b[39m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[39m    dask.base.compute\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m     (result,) \u001b[39m=\u001b[39m compute(\u001b[39mself\u001b[39;49m, traverse\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    315\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/mambaforge/envs/py311/lib/python3.11/site-packages/dask/base.py:599\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    596\u001b[0m     keys\u001b[39m.\u001b[39mappend(x\u001b[39m.\u001b[39m__dask_keys__())\n\u001b[1;32m    597\u001b[0m     postcomputes\u001b[39m.\u001b[39mappend(x\u001b[39m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m--> 599\u001b[0m results \u001b[39m=\u001b[39m schedule(dsk, keys, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    600\u001b[0m \u001b[39mreturn\u001b[39;00m repack([f(r, \u001b[39m*\u001b[39ma) \u001b[39mfor\u001b[39;00m r, (f, a) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m~/mambaforge/envs/py311/lib/python3.11/site-packages/dask/threaded.py:89\u001b[0m, in \u001b[0;36mget\u001b[0;34m(dsk, keys, cache, num_workers, pool, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(pool, multiprocessing\u001b[39m.\u001b[39mpool\u001b[39m.\u001b[39mPool):\n\u001b[1;32m     87\u001b[0m         pool \u001b[39m=\u001b[39m MultiprocessingPoolExecutor(pool)\n\u001b[0;32m---> 89\u001b[0m results \u001b[39m=\u001b[39m get_async(\n\u001b[1;32m     90\u001b[0m     pool\u001b[39m.\u001b[39;49msubmit,\n\u001b[1;32m     91\u001b[0m     pool\u001b[39m.\u001b[39;49m_max_workers,\n\u001b[1;32m     92\u001b[0m     dsk,\n\u001b[1;32m     93\u001b[0m     keys,\n\u001b[1;32m     94\u001b[0m     cache\u001b[39m=\u001b[39;49mcache,\n\u001b[1;32m     95\u001b[0m     get_id\u001b[39m=\u001b[39;49m_thread_get_id,\n\u001b[1;32m     96\u001b[0m     pack_exception\u001b[39m=\u001b[39;49mpack_exception,\n\u001b[1;32m     97\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m     98\u001b[0m )\n\u001b[1;32m    100\u001b[0m \u001b[39m# Cleanup pools associated to dead threads\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[39mwith\u001b[39;00m pools_lock:\n",
      "File \u001b[0;32m~/mambaforge/envs/py311/lib/python3.11/site-packages/dask/local.py:500\u001b[0m, in \u001b[0;36mget_async\u001b[0;34m(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[39mwhile\u001b[39;00m state[\u001b[39m\"\u001b[39m\u001b[39mwaiting\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mor\u001b[39;00m state[\u001b[39m\"\u001b[39m\u001b[39mready\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mor\u001b[39;00m state[\u001b[39m\"\u001b[39m\u001b[39mrunning\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    499\u001b[0m     fire_tasks(chunksize)\n\u001b[0;32m--> 500\u001b[0m     \u001b[39mfor\u001b[39;00m key, res_info, failed \u001b[39min\u001b[39;00m queue_get(queue)\u001b[39m.\u001b[39mresult():\n\u001b[1;32m    501\u001b[0m         \u001b[39mif\u001b[39;00m failed:\n\u001b[1;32m    502\u001b[0m             exc, tb \u001b[39m=\u001b[39m loads(res_info)\n",
      "File \u001b[0;32m~/mambaforge/envs/py311/lib/python3.11/site-packages/dask/local.py:137\u001b[0m, in \u001b[0;36mqueue_get\u001b[0;34m(q)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mqueue_get\u001b[39m(q):\n\u001b[0;32m--> 137\u001b[0m     \u001b[39mreturn\u001b[39;00m q\u001b[39m.\u001b[39;49mget()\n",
      "File \u001b[0;32m~/mambaforge/envs/py311/lib/python3.11/queue.py:171\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39melif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_qsize():\n\u001b[0;32m--> 171\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnot_empty\u001b[39m.\u001b[39;49mwait()\n\u001b[1;32m    172\u001b[0m \u001b[39melif\u001b[39;00m timeout \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    173\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m'\u001b[39m\u001b[39m must be a non-negative number\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/mambaforge/envs/py311/lib/python3.11/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "len(merged_ddf['person_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
